# 视觉组第三周考核任务

Author：@[liangbm3](https://github.com/liangbm3)

Revise:  @[skyswordx](https://github.com/skyswordx)

## 学习目标

+ 学习神经网络模型的训练过程
+ 学习制作自己的数据集
+ 了解launch和urdf模型
+ 学习如何使用手势控制小车

具体内容参加[学习指导](./get_started.md)  
附件文件在`source`文件夹下。
## 本周任务

1. 如果上一周没有完成yolov5环境的部署，请继续完成，并能成功运行`detect.py`
2. 我们给出了一个yolov5的一个训练结果，里面有权重和训练数据，该模型能够对五种不同手势进行分类，如图
    ![alt text](./image/val_batch0_labels.jpg)
    ![alt text](./image/val_batch1_labels.jpg)
    ![alt text](./image/val_batch2_labels.jpg)

    P.S: 看到这里不觉得 bm 学长很帅嘛 ~

    然后请你尝试：
    1. 自己拍摄一些类似的手势，然后使用`detect.py`进行预测，给出预测结果。
    2. 使用电脑摄像头/外置摄像头实时进行预测，给出截图
    **由于该权重是使用我们自己录制的数据集进行训练，且只由一个人录制，数据量较小，可能不具有鲁棒性，所以可能会出现准确度较低的情况**
3. 我们给出了自己录制的数据集，请你使用该数据集自己训练一个权重，给出训练过程。
4. 尝试自己录制个性手势，并用LabelImg标注，然后按照上述流程训练，给出训练过程。
5. 我们给出了一个小车模型和源码，在上述的任务中我们已经在yolov5中能够对手势进行识别，请尝试将手势信息发布在ros的一个话题中，然后小车订阅该话题，从而实现控制。

## 提示
+ 我们给出了一个`little_car`功能包和一个`car_turn`功能包，将这两个功能包放在你的ros工作空间的src文件夹下，然后执行`catkin_make`即可进行编译。
+ `little_car`功能包里面包含了小车的模型文件，同时在`src`文件夹下给出了控制小车的接口。在命令行中执行`roslaunch little_car little_car.launch`命令即可在RViz中看到小车模型，如图
    ![alt text](./image/car.png)
    这里需要了解一下ros中launch文件的使用方法。
+ `cart_turn`功能包是一个控制小车运动的功能包，`src`文件夹里面给出了如何控制小车的样例。这里是控制小车走一个矩形。在命令行中执行`roslaunch car_turn launch_turn.launch`即可看到效果。

## 评分标准
| 评分项                                                              | 积分值 |
| ------------------------------------------------------------------- | ------ |
| 能够使用我们给出的权重对上述五种手势进行推理                                                | 15     |
| 自己拍摄数据集进行标注并训练模型（附加）  | 20     |
| 使用我们的数据集训练一个模型                                              | 20     |
| 利用手势控制小车运动               | 40     |
| 能够编写launch文件启动节点 |15      |
|关于对本项目的理解 |10 |                                                         
    